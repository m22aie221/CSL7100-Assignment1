{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a6d7a7-693c-4fae-804f-3d92a1a30e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/rajesh/CSL7100/PySpark/spark-3.4.2-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cb5875-598a-4d74-be55-3795530d9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd73f68-a07d-47df-952e-38f0e681320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:09:38 WARN Utils: Your hostname, rajesh-pc resolves to a loopback address: 127.0.1.1; using 192.168.0.39 instead (on interface wlp1s0)\n",
      "26/02/13 09:09:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 09:09:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Gutenberg\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b294de5-c4a9-4f03-8dbc-ee29caaf0b99",
   "metadata": {},
   "source": [
    "Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fff0c64-66c7-4ddc-a496-04207087d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "books_df = (\n",
    "    spark.sparkContext\n",
    "         .wholeTextFiles(\"/home/rajesh/CSL7100/Assignment1/data/D184MB/*.txt\")   #Read all .txt files \n",
    "         .toDF([\"file_path\", \"text\"]) # Convert RDD into Dataframe with columnts file_path and text\n",
    "         .withColumn(\n",
    "             \"file_name\",  #create a new column \n",
    "             regexp_extract(\"file_path\", r\"([^/]+$)\", 1) # extract only file name from file_path  \n",
    "         )\n",
    "         .select(\"file_name\", \"text\")  #keep only file_name and text\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa9aacb-2472-4f50-bab0-42c79253ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb46655-99d9-4d99-b8a0-59defb7a1b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: 10.txt\n",
      "Text preview:\n",
      " The Project Gutenberg EBook of The King James Bible\n",
      "\n",
      "This eBook is for the use of anyone anywhere \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:10:47 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 2): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "row = books_df.first()\n",
    "\n",
    "print(\"File name:\", row.file_name)\n",
    "print(\"Text preview:\\n\", row.text[:100])   # first 1000 chars only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3a11f-bbc9-4e63-a1f4-46025da4d6bb",
   "metadata": {},
   "source": [
    "# 1. Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f1fc58-ad7f-4a0d-959b-7c7789a66044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "|file_name|title                                              |release_date                |language|encoding             |\n",
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "|10.txt   |The King James Bible                               |March 2, 2011 [EBook #10]   |English |ASCII                |\n",
      "|101.txt  |Hacker Crackdown                                   |January, 1994               |English |ASCII                |\n",
      "|102.txt  |The Tragedy of Pudd'nhead Wilson                   |January, 1994               |English |ASCII                |\n",
      "|103.txt  |Around the World in 80 Days                        |May 15, 2008 [EBook #103]   |English |ASCII                |\n",
      "|104.txt  |Franklin Delano Roosevelt's First Inaugural Address|May 14, 2008 [EBook #104]   |English |ASCII                |\n",
      "|105.txt  |Persuasion                                         |June 5, 2008 [EBook #105]   |English |ASCII                |\n",
      "|106.txt  |Jungle Tales of Tarzan                             |June 5, 2008 [EBook #106]   |English |ASCII                |\n",
      "|107.txt  |Far from the Madding Crowd                         |February, 1994  [eBook #107]|English |ISO-646-US (US-ASCII)|\n",
      "|108.txt  |The Return of Sherlock Holmes                      |July 8, 2007 [EBook #108]   |English |ASCII                |\n",
      "|109.txt  |Renascence and Other Poems                         |June 19, 2008 [EBook #109]  |English |ASCII                |\n",
      "|11.txt   |Alice's Adventures in Wonderland                   |March, 1994                 |English |ASCII                |\n",
      "|110.txt  |Tess of the d'Urbervilles                          |February, 1994  [eBook #110]|English |ISO-646-US (US-ASCII)|\n",
      "|111.txt  |Freckles                                           |March 8, 2006 [EBook #111]  |English |ASCII                |\n",
      "|112.txt  |Violists                                           |March, 1994                 |English |ASCII                |\n",
      "|113.txt  |The Secret Garden                                  |May 15, 2008 [EBook #113]   |English |ASCII                |\n",
      "|114.txt  |Tenniel Illustrations for Alice in Wonderland      |May 27, 2008 [EBook #114]   |English |ASCII                |\n",
      "|115.txt  |United States Census Figures back to 1630          |June 5, 2008 [EBook #115]   |English |ASCII                |\n",
      "|117.txt  |Beethoven's Fifth Symphony, in C-minor, Opus #67   |March 16, 2012 [EBook #117] |English |ASCII                |\n",
      "|118.txt  |Big Dummy's Guide to the Internet                  |March, 1994                 |English |ASCII                |\n",
      "|12.txt   |Through the Looking-Glass                          |February, 1991              |English |ASCII                |\n",
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:10:59 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 3): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#create a new data frame with columns\n",
    "books_df = (\n",
    "    df\n",
    "    .withColumn(\"title\",         #create\n",
    "        regexp_extract(\"text\", r\"(?i)Title:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"release_date\",\n",
    "        regexp_extract(\"text\", r\"(?i)Release Date:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"language\",\n",
    "        regexp_extract(\"text\", r\"(?i)Language:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"encoding\",\n",
    "        regexp_extract(\"text\", r\"(?i)Character set encoding:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .select(\"file_name\", \"title\", \"release_date\", \"language\", \"encoding\")\n",
    ")\n",
    "\n",
    "books_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3338233-3886-416a-bfde-fb484ad86baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, regexp_replace\n",
    "\n",
    "books_df = (\n",
    "    books_df\n",
    "    .withColumn(\"title\", trim(col(\"title\")))\n",
    "    .withColumn(\"language\", trim(col(\"language\")))\n",
    "    .withColumn(\n",
    "        \"release_date\",\n",
    "        trim(regexp_replace(\"release_date\", r\"\\[.*\", \"\"))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "276864ea-d0f7-4ed4-bc02-1a8f4f31545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+--------+--------+\n",
      "|file_name|               title| release_date|language|encoding|\n",
      "+---------+--------------------+-------------+--------+--------+\n",
      "|   10.txt|The King James Bible|March 2, 2011| English|   ASCII|\n",
      "|  101.txt|    Hacker Crackdown|January, 1994| English|   ASCII|\n",
      "|  102.txt|The Tragedy of Pu...|January, 1994| English|   ASCII|\n",
      "|  103.txt|Around the World ...| May 15, 2008| English|   ASCII|\n",
      "|  104.txt|Franklin Delano R...| May 14, 2008| English|   ASCII|\n",
      "+---------+--------------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:23:43 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 5): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "books_df.show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32687da-8539-4b0e-afb4-1c55463ace47",
   "metadata": {},
   "source": [
    "# 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81310d3-bf15-4948-b7a1-bf66fa9abd16",
   "metadata": {},
   "source": [
    "# Calculate the number of books released each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e39d545-ae33-493d-ad0f-886f7687c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+--------+--------+------------+\n",
      "|file_name|               title| release_date|language|encoding|release_year|\n",
      "+---------+--------------------+-------------+--------+--------+------------+\n",
      "|   10.txt|The King James Bible|March 2, 2011| English|   ASCII|        2011|\n",
      "|  101.txt|    Hacker Crackdown|January, 1994| English|   ASCII|        1994|\n",
      "|  102.txt|The Tragedy of Pu...|January, 1994| English|   ASCII|        1994|\n",
      "|  103.txt|Around the World ...| May 15, 2008| English|   ASCII|        2008|\n",
      "|  104.txt|Franklin Delano R...| May 14, 2008| English|   ASCII|        2008|\n",
      "+---------+--------------------+-------------+--------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:30:17 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 7 (TID 7): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "df_with_year = books_df.withColumn(\n",
    "    \"release_year\",\n",
    "    regexp_extract(\"release_date\", r\"(18|19|20)\\d{2}\", 0)  # extract a valid 4 digit year in the range 1800 to 2099\n",
    ")\n",
    "df_with_year.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31916e3-b140-401d-940a-0cdae3aef804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|release_year|num_books|\n",
      "+------------+---------+\n",
      "|        1975|        1|\n",
      "|        1978|        1|\n",
      "|        1979|        1|\n",
      "|        1991|        7|\n",
      "|        1992|       19|\n",
      "+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "books_per_year = (\n",
    "    df_with_year\n",
    "    .filter(\"release_year != ''\")        #remove empty years\n",
    "    .groupBy(\"release_year\")             # group by year\n",
    "    .agg(count(\"*\").alias(\"num_books\"))  # count bootks per year\n",
    "    .orderBy(\"release_year\")             # sort by release year\n",
    ")\n",
    "\n",
    "books_per_year.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3c28f-5f41-4aec-82ee-0598959fe3c3",
   "metadata": {},
   "source": [
    "Find the most common language in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd309c86-04fc-4902-bac0-bf66f888a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|language|num_books|\n",
      "+--------+---------+\n",
      "|English |404      |\n",
      "|Latin   |6        |\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, trim, col\n",
    "\n",
    "most_common_language = (\n",
    "    books_df\n",
    "    .withColumn(\"language\", trim(col(\"language\")))   #remove extra spaces from language values\n",
    "    .filter(col(\"language\").rlike(\"^[A-Za-z ]+$\")) # language should have only alphabet characters, filter out the empty values, numbers etc.\n",
    "    .groupBy(\"language\")  #group by same language\n",
    "    .agg(count(\"*\").alias(\"num_books\"))  #sum by language\n",
    "    .orderBy(col(\"num_books\").desc()) #sort by the num_books\n",
    ")\n",
    "\n",
    "most_common_language.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad6d3b8f-583f-4741-916a-8d1941fe9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|title                                              |\n",
      "+---------------------------------------------------+\n",
      "|The King James Bible                               |\n",
      "|Hacker Crackdown                                   |\n",
      "|The Tragedy of Pudd'nhead Wilson                   |\n",
      "|Around the World in 80 Days                        |\n",
      "|Franklin Delano Roosevelt's First Inaugural Address|\n",
      "+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 09:56:39 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 20 (TID 20): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, trim, col\n",
    "\n",
    "book_title = (\n",
    "    books_df\n",
    "    .withColumn(\"title\", trim(col(\"title\")))   #clean title column\n",
    "    .filter(col(\"title\") != \"\")  #remove empty titles\n",
    "    .select(\"title\") #keep only title column\n",
    ")\n",
    "\n",
    "\n",
    "book_title.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31cf47-d561-47d0-a3b2-109220e1f1c0",
   "metadata": {},
   "source": [
    "Determine the average length of book titles (in characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49314166-584c-49e6-90c5-959df6ab35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|avg_title_length_chars|\n",
      "+----------------------+\n",
      "|    22.829268292682926|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, avg, col, trim\n",
    "\n",
    "avg_title_length = (\n",
    "    book_title\n",
    "    .withColumn(\"title_length\", length(col(\"title\")))  #compute the title length\n",
    "    .agg(avg(\"title_length\").alias(\"avg_title_length_chars\")) #avg out the title length\n",
    ")\n",
    "\n",
    "avg_title_length.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60636c9d-c9da-4abf-b682-c67d91a3c740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4ece8d9-ed91-4b4e-802f-a6e260dc46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509b04d-32b3-4b17-ab4e-6ed025b09762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973fee9-6c37-4b79-8347-929df7d9dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
