{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a6d7a7-693c-4fae-804f-3d92a1a30e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/rajesh/CSL7100/PySpark/spark-3.4.2-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cb5875-598a-4d74-be55-3795530d9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661d5311-aa14-4449-9bbb-4e6998f9621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 14:26:30 WARN Utils: Your hostname, rajesh-pc resolves to a loopback address: 127.0.1.1; using 192.168.0.39 instead (on interface wlp1s0)\n",
      "26/02/13 14:26:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 14:26:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/13 14:26:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame-Gutenberg-Similarity\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"24\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b294de5-c4a9-4f03-8dbc-ee29caaf0b99",
   "metadata": {},
   "source": [
    "### Using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fff0c64-66c7-4ddc-a496-04207087d52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "books_df = (\n",
    "    spark.sparkContext\n",
    "         .wholeTextFiles(\"/home/rajesh/CSL7100/Assignment1/data/D184MB/*.txt\") #reads the whole text files from specified path\n",
    "         .toDF([\"file_path\", \"text\"]) #convert RDD to data frame\n",
    "         .withColumn(\n",
    "             \"file_name\",  # add a new column calle file_name and extract it using regular expression\n",
    "             regexp_extract(\"file_path\", r\"([^/]+$)\", 1)\n",
    "         )\n",
    "         .select(\"file_name\", \"text\") #keep file_name and text column\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa9aacb-2472-4f50-bab0-42c79253ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb46655-99d9-4d99-b8a0-59defb7a1b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: 10.txt\n",
      "Text preview:\n",
      " The Project Gutenberg EBook of The King James Bible\n",
      "\n",
      "This eBook is for the use of anyone anywhere \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 14:26:50 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 1): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "row = books_df.first()\n",
    "\n",
    "print(\"File name:\", row.file_name)\n",
    "print(\"Text preview:\\n\", row.text[:100])   # first 1000 chars only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22fb193-7f3e-4774-8a1d-fa3c3daa6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import builtin function for cleaning and formating the text\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    lower,\n",
    "    col,\n",
    "    split,\n",
    ")\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import split, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f1fc58-ad7f-4a0d-959b-7c7789a66044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import split, expr\n",
    "\n",
    "books_clean = (\n",
    "    books_df\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(                  #remove the header\n",
    "            col(\"text\"),\n",
    "            r\"(?is)^.*?\\*\\*\\*\\s*START OF.*?\\*\\*\\*\",\n",
    "            \"\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(                 #remove the footer\n",
    "            col(\"clean_text\"),\n",
    "            r\"(?is)\\*\\*\\*\\s*END OF.*?\\*\\*\\*.*$\",\n",
    "            \"\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"clean_text\", lower(col(\"clean_text\")))         #convert the text to lower case\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(col(\"clean_text\"), r\"[^a-z\\s]\", \" \")     #remove the punctuations\n",
    "    )\n",
    "    .withColumn(\"words\", split(col(\"clean_text\"), r\"\\s+\"))      #tokenize into words array\n",
    ")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"tokens\")\n",
    "books_clean = remover.transform(books_clean)\n",
    "\n",
    "books_clean = books_clean.select(\"file_name\", \"tokens\")  # keep file_name and tokenized words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d521a06f-e53b-412c-ad46-ba78ddab35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book count =  425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 14:27:00 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 5): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "|file_name|                                                                                              tokens|\n",
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "|   10.txt|[, old, testament, king, james, version, bible, first, book, moses, called, genesis, beginning, g...|\n",
      "|  101.txt|[, hacker, crackdown, law, disorder, electronic, frontier, bruce, sterling, contents, preface, el...|\n",
      "|  102.txt|[, produced, anonymous, volunteer, tragedy, pudd, nhead, wilson, mark, twain, whisper, reader, ch...|\n",
      "|  103.txt|[, around, world, eighty, days, contents, chapter, phileas, fogg, passepartout, accept, one, mast...|\n",
      "|  104.txt|[, inaugural, address, franklin, delano, roosevelt, given, washington, d, c, march, th, president...|\n",
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"book count = \", books_clean.count())\n",
    "books_clean.show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433d97cf-ed37-4780-9deb-f54e2af8f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[file_name: string, text: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148992d9-f80c-48ba-9192-18d91c639c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.unpersist(blocking=True) if books_df.is_cached else None\n",
    "del books_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab4f612-e0b5-4603-89a5-fd2bc24a032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731707a3-5e93-4182-b3b4-c2b728fb4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_clean = books_clean.repartition(6, \"file_name\").cache()\n",
    "books_clean.count()   # materialize cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0c440-7f83-451f-af13-c965b651453d",
   "metadata": {},
   "source": [
    "2. TF- IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c38a43-2750-45ef-8c3a-2163d99b6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import udf, col, lit, explode\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import explode, countDistinct, log, lit, col, count, size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47288e95-5ace-4d03-b0bf-8abe0fc3ed77",
   "metadata": {},
   "source": [
    "### Calculate the Term Frequency (TF) of each word in each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9adfe3-a1e7-4707-bddc-8d6b5e3d3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, count\n",
    "\n",
    "tf_df = (\n",
    "    books_clean\n",
    "    .select(\"file_name\", explode(col(\"tokens\")).alias(\"word\"))\n",
    "    .groupBy(\"file_name\", \"word\")\n",
    "    .agg(count(\"*\").alias(\"term_count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07dac251-74ec-4427-b445-51cc351924e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+\n",
      "|file_name|word     |term_count|\n",
      "+---------+---------+----------+\n",
      "|102.txt  |en       |344       |\n",
      "|102.txt  |de       |298       |\n",
      "|102.txt  |tom      |294       |\n",
      "|102.txt  |said     |252       |\n",
      "|102.txt  |wilson   |223       |\n",
      "|102.txt  |one      |202       |\n",
      "|102.txt  |dat      |156       |\n",
      "|102.txt  |got      |136       |\n",
      "|102.txt  |man      |117       |\n",
      "|102.txt  |old      |103       |\n",
      "|112.txt  |one      |92        |\n",
      "|112.txt  |gretchen |84        |\n",
      "|112.txt  |jurgen   |76        |\n",
      "|112.txt  |professor|65        |\n",
      "|112.txt  |back     |54        |\n",
      "|112.txt  |said     |48        |\n",
      "|112.txt  |like     |46        |\n",
      "|112.txt  |time     |46        |\n",
      "|112.txt  |hand     |45        |\n",
      "|112.txt  |viola    |42        |\n",
      "+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"file_name\").orderBy(desc(\"term_count\"))\n",
    "\n",
    "top10_per_book = (\n",
    "    tf_df\n",
    "    .withColumn(\"rank\", row_number().over(window_spec))  # assign rank per book\n",
    "    .filter(\"rank <= 10\")  # keep top 10\n",
    "    .drop(\"rank\")  # remove helper column\n",
    ")\n",
    "\n",
    "top10_per_book.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01798262-856a-4b3f-8934-5a3d2517742f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893d689a-3c61-4c73-96a0-2ac18b36fc89",
   "metadata": {},
   "source": [
    "### Calculate the Inverse Document Frequency (IDF) for each word across all books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69aba7c9-7683-437d-9dfc-a574f0f26ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files counts N  425\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "N = tf_df.select(\"file_name\").distinct().count()\n",
    "\n",
    "print(\"Total files counts N \", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def249bf-317a-4955-957e-308005f384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 6) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|word        |doc_freq|\n",
      "+------------+--------+\n",
      "|spirit      |339     |\n",
      "|doubt       |342     |\n",
      "|matters     |299     |\n",
      "|every       |395     |\n",
      "|rewritten   |15      |\n",
      "|eye         |347     |\n",
      "|ago         |352     |\n",
      "|still       |387     |\n",
      "|del         |44      |\n",
      "|build       |245     |\n",
      "|old         |385     |\n",
      "|honeysuckles|4       |\n",
      "|palings     |21      |\n",
      "|stood       |358     |\n",
      "|boxes       |169     |\n",
      "|grew        |324     |\n",
      "|made        |396     |\n",
      "|symbol      |98      |\n",
      "|chief       |302     |\n",
      "|lofty       |168     |\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Number of files in which a word appeared\n",
    "\n",
    "df_word = (\n",
    "    tf_df            #term frquency df\n",
    "    .groupBy(\"word\")  #group by word\n",
    "    .agg(countDistinct(\"file_name\").alias(\"doc_freq\"))  #count unique documents\n",
    ")\n",
    "\n",
    "df_word.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1813ff84-9069-4db6-800f-eaf708ad3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute IDF \n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "idf_df = df_word.withColumn(\n",
    "    \"idf\",\n",
    "    log(lit(N) / (col(\"doc_freq\") + lit(1)))    # compute log(N/df)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718b8709-4206-4408-b68c-5d7c7dc06603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 6) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------------+\n",
      "|word        |doc_freq|idf                |\n",
      "+------------+--------+-------------------+\n",
      "|spirit      |339     |0.22314355131420976|\n",
      "|doubt       |342     |0.21435872175847698|\n",
      "|matters     |299     |0.3483066942682158 |\n",
      "|every       |395     |0.07067495766993637|\n",
      "|rewritten   |15      |3.2795004466846356 |\n",
      "|eye         |347     |0.19988668914994248|\n",
      "|ago         |352     |0.1856211119911201 |\n",
      "|still       |387     |0.09108382930114349|\n",
      "|del         |44      |2.245426679154097  |\n",
      "|build       |245     |0.5467576329920539 |\n",
      "|old         |385     |0.0962517994595859 |\n",
      "|honeysuckles|4       |4.442651256490317  |\n",
      "|palings     |21      |2.961046715566101  |\n",
      "|stood       |358     |0.16876678043613805|\n",
      "|boxes       |169     |0.9162907318741551 |\n",
      "|grew        |324     |0.26826398659467937|\n",
      "|made        |396     |0.0681528882372264 |\n",
      "|symbol      |98      |1.456969318789827  |\n",
      "|chief       |302     |0.3383563634150477 |\n",
      "|lofty       |168     |0.9221904540013435 |\n",
      "+------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "idf_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a9da7-0271-4119-a5a4-4229ee616951",
   "metadata": {},
   "source": [
    "### Compute the TF-IDF score for each word in each book (TF * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d250c74-96e3-4e33-9f05-88ed572c773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tf_idf_df = (\n",
    "    tf_df     # term frequency data\n",
    "    .join(idf_df.select(\"word\", \"idf\"),  # join with IDF\n",
    "          on=\"word\", how=\"inner\")  # join on word\n",
    "    .withColumn(\"tf_idf\", # create TF-IDF column\n",
    "                col(\"term_count\") * col(\"idf\"))   # multiply TF and IDF\n",
    "    .select(\"file_name\", \"word\", \"tf_idf\")  # select final columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e25f7bad-fd8a-46ce-b98d-742c749489ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=========>                                                (1 + 5) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|file_name|word       |tf_idf            |\n",
      "+---------+-----------+------------------+\n",
      "|38.txt   |aaargh     |10.717883976728944|\n",
      "|200.txt  |aad        |5.358941988364472 |\n",
      "|80.txt   |aag        |5.358941988364472 |\n",
      "|200.txt  |aak        |5.358941988364472 |\n",
      "|124.txt  |abadias    |5.358941988364472 |\n",
      "|14.txt   |abaiang    |4.260329699696362 |\n",
      "|25.txt   |abaiang    |4.260329699696362 |\n",
      "|48.txt   |abaiang    |4.260329699696362 |\n",
      "|180.txt  |abaiang    |4.260329699696362 |\n",
      "|87.txt   |abaiang    |4.260329699696362 |\n",
      "|200.txt  |abailard   |4.953476880256307 |\n",
      "|267.txt  |abailard   |4.953476880256307 |\n",
      "|200.txt  |abaissement|5.358941988364472 |\n",
      "|24.txt   |abandonedly|5.358941988364472 |\n",
      "|200.txt  |abaris     |5.358941988364472 |\n",
      "|228.txt  |abas       |24.767384401281536|\n",
      "|227.txt  |abas       |9.906953760512614 |\n",
      "|102.txt  |abashed    |1.7346010553881064|\n",
      "|172.txt  |abashed    |1.7346010553881064|\n",
      "|224.txt  |abashed    |3.4692021107762128|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "tf_idf_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf40fb6-5486-47e8-9840-9f3fa11eef51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2775e96e-6a73-48a4-9d1d-acaa1c7188ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+\n",
      "|file_name|    word|  tf|\n",
      "+---------+--------+----+\n",
      "|   10.txt|     one|1969|\n",
      "|   10.txt|    make|1056|\n",
      "|   10.txt|    rain| 102|\n",
      "|   10.txt| watered|  11|\n",
      "|   10.txt|breathed|   4|\n",
      "|   10.txt|   river| 179|\n",
      "|   10.txt|  parted|  12|\n",
      "|   10.txt|  freely|  17|\n",
      "|   10.txt|    gave| 465|\n",
      "|   10.txt|   naked|  47|\n",
      "+---------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_tf = (\n",
    "    books_clean\n",
    "    .select(\"file_name\", explode(col(\"tokens\")).alias(\"word\"))\n",
    "    .groupBy(\"file_name\", \"word\")\n",
    "    .agg(count(\"*\").alias(\"tf\"))\n",
    ")\n",
    "\n",
    "df_tf.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7faba26-c30e-4e47-ade5-ce5ffed7aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF Scores for book '10.txt' (Top 20) ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TF Scores for book \u001b[39m\u001b[33m'\u001b[39m\u001b[33m10.txt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (Top 20) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtf_df\u001b[49m.filter(col(\u001b[33m\"\u001b[39m\u001b[33mfile_name\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m10.txt\u001b[39m\u001b[33m\"\u001b[39m).orderBy(col(\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m).desc()).show(\u001b[32m20\u001b[39m, truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TF Scores for book '10.txt' (Top 20) ===\")\n",
    "tf_df.filter(col(\"file_name\") == \"10.txt\").orderBy(col(\"tf\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd9082-3364-4f88-95b5-31e1c2a48a66",
   "metadata": {},
   "source": [
    "### Step 2: Calculate IDF for each word across all books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92fee459-d157-4249-9cde-ab543d33b2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Calculating IDF (Inverse Document Frequency) ===\n",
      "Total number of books: 425\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Calculating IDF (Inverse Document Frequency) ===\")\n",
    "total_books = books_clean.count()\n",
    "print(f\"Total number of books: {total_books}\")\n",
    "\n",
    "idf_df = (\n",
    "    books_clean\n",
    "    .select(\"file_name\", explode(col(\"tokens\")).alias(\"word\"))\n",
    "    .groupBy(\"word\")\n",
    "    .agg(countDistinct(\"file_name\").alias(\"num_books\"))\n",
    "    .withColumn(\"idf\", log(lit(total_books) / col(\"num_books\")))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d462207-0299-448e-a005-4b5955d9bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IDF Scores (Top 20 rarest words) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:========================>                                 (3 + 4) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------------+\n",
      "|word            |num_books|idf              |\n",
      "+----------------+---------+-----------------+\n",
      "|choijilsurengiyn|1        |6.052089168924417|\n",
      "|susimilkie      |1        |6.052089168924417|\n",
      "|hydrogene       |1        |6.052089168924417|\n",
      "|gutzkow         |1        |6.052089168924417|\n",
      "|christines      |1        |6.052089168924417|\n",
      "|weatherboards   |1        |6.052089168924417|\n",
      "|baldi           |1        |6.052089168924417|\n",
      "|prosecutorial   |1        |6.052089168924417|\n",
      "|isoceles        |1        |6.052089168924417|\n",
      "|trampish        |1        |6.052089168924417|\n",
      "|unsexual        |1        |6.052089168924417|\n",
      "|deadlit         |1        |6.052089168924417|\n",
      "|nola            |1        |6.052089168924417|\n",
      "|undertrained    |1        |6.052089168924417|\n",
      "|quintessentially|1        |6.052089168924417|\n",
      "|warriorship     |1        |6.052089168924417|\n",
      "|agnibilckrou    |1        |6.052089168924417|\n",
      "|chailey         |1        |6.052089168924417|\n",
      "|suddarth        |1        |6.052089168924417|\n",
      "|spewers         |1        |6.052089168924417|\n",
      "+----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n=== IDF Scores (Top 20 rarest words) ===\")\n",
    "idf_df.select(\"word\", \"num_books\", \"idf\").orderBy(col(\"idf\").desc()).show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca345368-84c6-4a96-b843-f2ee3e82ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IDF Scores (Top 20 most common words) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+\n",
      "|word     |num_books|idf                 |\n",
      "+---------+---------+--------------------+\n",
      "|         |425      |0.0                 |\n",
      "|end      |423      |0.004716989878138867|\n",
      "|project  |405      |0.048202101817877686|\n",
      "|gutenberg|402      |0.05563708030539568 |\n",
      "|time     |401      |0.058127741617847686|\n",
      "|one      |401      |0.058127741617847686|\n",
      "|may      |400      |0.06062462181643484 |\n",
      "|first    |398      |0.06563716363997904 |\n",
      "|made     |396      |0.07067495766993637 |\n",
      "|well     |395      |0.07320340402329492 |\n",
      "|every    |395      |0.07320340402329492 |\n",
      "|even     |394      |0.07573825962648298 |\n",
      "|many     |394      |0.07573825962648298 |\n",
      "|two      |394      |0.07573825962648298 |\n",
      "|make     |394      |0.07573825962648298 |\n",
      "|way      |393      |0.07827955705515566 |\n",
      "|long     |392      |0.08082732913395425 |\n",
      "|come     |392      |0.08082732913395425 |\n",
      "|place    |392      |0.08082732913395425 |\n",
      "|must     |392      |0.08082732913395425 |\n",
      "+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n=== IDF Scores (Top 20 most common words) ===\")\n",
    "idf_df.select(\"word\", \"num_books\", \"idf\").orderBy(col(\"idf\").asc()).show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07520b15-bb9a-44c9-a564-f1e9e0a0d794",
   "metadata": {},
   "source": [
    "# Step 3: Calculate TF-IDF = TF Ã— IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d86ed26-a581-4195-9477-429881479b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Calculating TF-IDF ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Calculating TF-IDF ===\")\n",
    "tfidf_df = (\n",
    "    tf_df\n",
    "    .join(idf_df, \"word\")\n",
    "    .withColumn(\"tfidf\", col(\"tf\") * col(\"idf\"))\n",
    "    .select(\"file_name\", \"word\", \"word_count\", \"total_words\", \"tf\", \"num_books\", \"idf\", \"tfidf\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "259d0809-221c-4a2f-93a3-ba49acd61bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF, IDF, TF-IDF Together (Top 20 highest TF-IDF - all books) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 12:25:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "26/02/13 12:25:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "26/02/13 12:25:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 50:=================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+------------------+-------------------+\n",
      "|file_name|word     |tf                  |idf               |tfidf              |\n",
      "+---------+---------+--------------------+------------------+-------------------+\n",
      "|302.txt  |digits   |0.9631782945736435  |3.48713981146288  |3.3587373765446733 |\n",
      "|212.txt  |digits   |0.4372623574144487  |3.48713981146288  |1.524794974594035  |\n",
      "|115.txt  |byear    |0.1945780498469611  |6.052089168924417 |1.1776037079892285 |\n",
      "|115.txt  |ayear    |0.1945780498469611  |6.052089168924417 |1.1776037079892285 |\n",
      "|88.txt   |est      |0.6373504652193177  |1.6094379124341003|1.0257760022314812 |\n",
      "|15.txt   |moby     |0.13195098963242224 |4.442651256490317 |0.5862122298856214 |\n",
      "|255.txt  |qread    |0.08196721311475409 |6.052089168924417 |0.4960728826987227 |\n",
      "|239.txt  |png      |0.07142857142857142 |6.052089168924417 |0.4322920834946012 |\n",
      "|239.txt  |radar    |0.10714285714285714 |3.8548645915881976|0.41302120624159255|\n",
      "|276.txt  |zurflieh |0.058823529411764705|6.052089168924417 |0.3560052452308481 |\n",
      "|15.txt   |txt      |0.12723845428840716 |2.6847933389379426|0.34160895453027545|\n",
      "|239.txt  |jpg      |0.07142857142857142 |4.665794807804526 |0.3332710577003233 |\n",
      "|117.txt  |pawlicki |0.06976744186046512 |4.665794807804526 |0.3255205679863623 |\n",
      "|117.txt  |geof     |0.06976744186046512 |4.665794807804526 |0.3255205679863623 |\n",
      "|156.txt  |pawlicki |0.06976744186046512 |4.665794807804526 |0.3255205679863623 |\n",
      "|156.txt  |geof     |0.06976744186046512 |4.665794807804526 |0.3255205679863623 |\n",
      "|65.txt   |printf   |0.05714285714285714 |5.358941988364472 |0.30622525647796983|\n",
      "|279.txt  |gif      |0.09090909090909091 |3.2795004466846356|0.2981364042440578 |\n",
      "|303.txt  |homepages|0.04878048780487805 |6.052089168924417 |0.29522386189875205|\n",
      "|156.txt  |beethoven|0.09302325581395349 |3.1076501897579765|0.2890837385821373 |\n",
      "+---------+---------+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TF, IDF, TF-IDF Together (Top 20 highest TF-IDF - all books) ===\")\n",
    "tfidf_df.select(\"file_name\", \"word\", \"tf\", \"idf\", \"tfidf\").orderBy(col(\"tfidf\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf7251-1304-477c-bb50-7714bcc4464b",
   "metadata": {},
   "source": [
    "print(\"\\n=== TF, IDF, TF-IDF for book '10.txt' (Top 20) ===\")\n",
    "tfidf_df.filter(col(\"file_name\") == \"10.txt\").select(\"word\", \"tf\", \"idf\", \"tfidf\").orderBy(col(\"tfidf\").desc()).show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6bfaf0-6d7c-459c-b701-31d8c1ddb01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complete Details for book '10.txt' (Top 20) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+-----------+---------------------+---------+-------------------+---------------------+\n",
      "|file_name|word     |word_count|total_words|tf                   |num_books|idf                |tfidf                |\n",
      "+---------+---------+----------+-----------+---------------------+---------+-------------------+---------------------+\n",
      "|10.txt   |unto     |8997      |375131     |0.02398362172147863  |166      |0.9401013805678735 |0.0225470358913797   |\n",
      "|10.txt   |israel   |2575      |375131     |0.006864268748783758 |68       |1.8325814637483102 |0.01257933167120792  |\n",
      "|10.txt   |thou     |5474      |375131     |0.01459223577896788  |188      |0.8156472060944675 |0.011902116343786876 |\n",
      "|10.txt   |thy      |4600      |375131     |0.012262383007536034 |178      |0.8703056186323317 |0.01067202082928024  |\n",
      "|10.txt   |thee     |3827      |375131     |0.010201769515182696 |171      |0.910425612421757  |0.009287952258645817 |\n",
      "|10.txt   |saith    |1262      |375131     |0.003364158120763147 |39       |2.3885275227947704 |0.00803538426247631  |\n",
      "|10.txt   |hath     |2264      |375131     |0.006035225028056866 |124      |1.2318076033193799 |0.007434236077303865 |\n",
      "|10.txt   |judah    |816       |375131     |0.0021752401161194354|16       |3.2795004466846356 |0.007133700932460027 |\n",
      "|10.txt   |ye       |3983      |375131     |0.010617624243264352 |218      |0.6675941061353278 |0.007088263365962851 |\n",
      "|10.txt   |shalt    |1616      |375131     |0.004307828465256137 |83       |1.633248561127819  |0.007035754642465046 |\n",
      "|10.txt   |lord     |7964      |375131     |0.021229917015655863 |311      |0.31229625674518263|0.006630023615000185 |\n",
      "|10.txt   |hast     |1070      |375131     |0.0028523369169703382|93       |1.519489675771161  |0.004334096497157372 |\n",
      "|10.txt   |moses    |852       |375131     |0.002271206591830587 |68       |1.8325814637483102 |0.004162171100531708 |\n",
      "|10.txt   |thereof  |906       |375131     |0.0024151563053973145|76       |1.7213558286380857 |0.004157343383367692 |\n",
      "|10.txt   |jesus    |983       |375131     |0.002620417934001722 |88       |1.5747523544462103 |0.004126509311202287 |\n",
      "|10.txt   |jerusalem|814       |375131     |0.002169908645246594 |67       |1.8473965495334508 |0.004008681744031363 |\n",
      "|10.txt   |thine    |938       |375131     |0.0025004598393627824|103      |1.417360180694781  |0.0035440522097392765|\n",
      "|10.txt   |shall    |9838      |375131     |0.026225505223508586 |374      |0.127833371509885  |0.0033524947522712025|\n",
      "|10.txt   |spake    |587       |375131     |0.0015647867011790548|56       |2.0267374781892675 |0.0031714118526517403|\n",
      "|10.txt   |levites  |265       |375131     |7.064198906515324E-4 |6        |4.260329699696362  |0.0030095816405989796|\n",
      "+---------+---------+----------+-----------+---------------------+---------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Complete Details for book '10.txt' (Top 20) ===\")\n",
    "tfidf_df.filter(col(\"file_name\") == \"10.txt\").orderBy(col(\"tfidf\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a740b-e596-42c0-911b-35b9a41161c6",
   "metadata": {},
   "source": [
    "## Step 4 Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a8d632d-a4be-4060-b780-0d148e16e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary Statistics ===\n",
      "TF Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                  tf|\n",
      "+-------+--------------------+\n",
      "|  count|             2409637|\n",
      "|   mean|1.763751137619456...|\n",
      "| stddev|0.001165423237910...|\n",
      "|    min|1.412114245690933...|\n",
      "|    max|  0.9631782945736435|\n",
      "+-------+--------------------+\n",
      "\n",
      "IDF Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               idf|\n",
      "+-------+------------------+\n",
      "|  count|            210899|\n",
      "|   mean| 5.129266120670784|\n",
      "| stddev|1.3494211538327372|\n",
      "|    min|               0.0|\n",
      "|    max| 6.052089168924417|\n",
      "+-------+------------------+\n",
      "\n",
      "TF-IDF Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 12:27:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "26/02/13 12:27:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 105:================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|               tfidf|\n",
      "+-------+--------------------+\n",
      "|  count|             2409637|\n",
      "|   mean|1.881274890776004E-4|\n",
      "| stddev|0.003196774853135...|\n",
      "|    min|                 0.0|\n",
      "|    max|  3.3587373765446733|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 4: Summary Statistics\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(\"TF Statistics:\")\n",
    "tf_df.select(\"tf\").describe().show()\n",
    "\n",
    "print(\"IDF Statistics:\")\n",
    "idf_df.select(\"idf\").describe().show()\n",
    "\n",
    "print(\"TF-IDF Statistics:\")\n",
    "tfidf_df.select(\"tfidf\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824c17d-30b6-4912-a05c-84ef4d3fe1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ece8d9-ed91-4b4e-802f-a6e260dc46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf51b5-9094-408d-ba30-658f73d56f97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d29b3-ca4e-4a56-b618-3284c3fe9192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd55c49-693e-40c7-bf5a-903ce796f7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f4715-568b-4325-900d-85a3caa843fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2515b77-bff6-4b76-bede-55aeb7e6096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509b04d-32b3-4b17-ab4e-6ed025b09762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973fee9-6c37-4b79-8347-929df7d9dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
