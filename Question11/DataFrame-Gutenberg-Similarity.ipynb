{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68a6d7a7-693c-4fae-804f-3d92a1a30e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/rajesh/CSL7100/PySpark/spark-3.4.2-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9cb5875-598a-4d74-be55-3795530d9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "661d5311-aa14-4449-9bbb-4e6998f9621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame-Gutenberg-Network\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"24\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b294de5-c4a9-4f03-8dbc-ee29caaf0b99",
   "metadata": {},
   "source": [
    "### Using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fff0c64-66c7-4ddc-a496-04207087d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "books_df = (\n",
    "    spark.sparkContext\n",
    "         .wholeTextFiles(\"/home/rajesh/CSL7100/Assignment1/data/D184MB/*.txt\") #reads the whole text files from specified path\n",
    "         .toDF([\"file_path\", \"text\"]) #convert RDD to data frame\n",
    "         .withColumn(\n",
    "             \"file_name\",  # add a new column calle file_name and extract it using regular expression\n",
    "             regexp_extract(\"file_path\", r\"([^/]+$)\", 1)\n",
    "         )\n",
    "         .select(\"file_name\", \"text\") #keep file_name and text column\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b0f7d58-c787-4c84-8d73-fa51a9d8fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "|file_name|title                                              |release_date                |language|encoding             |\n",
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "|10.txt   |The King James Bible                               |March 2, 2011 [EBook #10]   |English |ASCII                |\n",
      "|101.txt  |Hacker Crackdown                                   |January, 1994               |English |ASCII                |\n",
      "|102.txt  |The Tragedy of Pudd'nhead Wilson                   |January, 1994               |English |ASCII                |\n",
      "|103.txt  |Around the World in 80 Days                        |May 15, 2008 [EBook #103]   |English |ASCII                |\n",
      "|104.txt  |Franklin Delano Roosevelt's First Inaugural Address|May 14, 2008 [EBook #104]   |English |ASCII                |\n",
      "|105.txt  |Persuasion                                         |June 5, 2008 [EBook #105]   |English |ASCII                |\n",
      "|106.txt  |Jungle Tales of Tarzan                             |June 5, 2008 [EBook #106]   |English |ASCII                |\n",
      "|107.txt  |Far from the Madding Crowd                         |February, 1994  [eBook #107]|English |ISO-646-US (US-ASCII)|\n",
      "|108.txt  |The Return of Sherlock Holmes                      |July 8, 2007 [EBook #108]   |English |ASCII                |\n",
      "|109.txt  |Renascence and Other Poems                         |June 19, 2008 [EBook #109]  |English |ASCII                |\n",
      "|11.txt   |Alice's Adventures in Wonderland                   |March, 1994                 |English |ASCII                |\n",
      "|110.txt  |Tess of the d'Urbervilles                          |February, 1994  [eBook #110]|English |ISO-646-US (US-ASCII)|\n",
      "|111.txt  |Freckles                                           |March 8, 2006 [EBook #111]  |English |ASCII                |\n",
      "|112.txt  |Violists                                           |March, 1994                 |English |ASCII                |\n",
      "|113.txt  |The Secret Garden                                  |May 15, 2008 [EBook #113]   |English |ASCII                |\n",
      "|114.txt  |Tenniel Illustrations for Alice in Wonderland      |May 27, 2008 [EBook #114]   |English |ASCII                |\n",
      "|115.txt  |United States Census Figures back to 1630          |June 5, 2008 [EBook #115]   |English |ASCII                |\n",
      "|117.txt  |Beethoven's Fifth Symphony, in C-minor, Opus #67   |March 16, 2012 [EBook #117] |English |ASCII                |\n",
      "|118.txt  |Big Dummy's Guide to the Internet                  |March, 1994                 |English |ASCII                |\n",
      "|12.txt   |Through the Looking-Glass                          |February, 1991              |English |ASCII                |\n",
      "+---------+---------------------------------------------------+----------------------------+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 16:12:36 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 5): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#create a new data frame with columns\n",
    "books_df = (\n",
    "    books_df\n",
    "    .withColumn(\"title\",         #create\n",
    "        regexp_extract(\"text\", r\"(?i)Title:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"release_date\",\n",
    "        regexp_extract(\"text\", r\"(?i)Release Date:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"language\",\n",
    "        regexp_extract(\"text\", r\"(?i)Language:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .withColumn(\"encoding\",\n",
    "        regexp_extract(\"text\", r\"(?i)Character set encoding:\\s*(.*)\", 1)\n",
    "    )\n",
    "    .select(\"file_name\", \"title\", \"release_date\", \"language\", \"encoding\")\n",
    ")\n",
    "\n",
    "books_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfa9aacb-2472-4f50-bab0-42c79253ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bb46655-99d9-4d99-b8a0-59defb7a1b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: 10.txt\n",
      "Text preview:\n",
      " The Project Gutenberg EBook of The King James Bible\n",
      "\n",
      "This eBook is for the use of anyone anywhere \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 16:01:36 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 1): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "row = books_df.first()\n",
    "\n",
    "print(\"File name:\", row.file_name)\n",
    "print(\"Text preview:\\n\", row.text[:100])   # first 1000 chars only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a22fb193-7f3e-4774-8a1d-fa3c3daa6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import builtin function for cleaning and formating the text\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    lower,\n",
    "    col,\n",
    "    split,\n",
    ")\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import split, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14f1fc58-ad7f-4a0d-959b-7c7789a66044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import split, expr\n",
    "\n",
    "books_clean = (\n",
    "    books_df\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(                  #remove the header\n",
    "            col(\"text\"),\n",
    "            r\"(?is)^.*?\\*\\*\\*\\s*START OF.*?\\*\\*\\*\",\n",
    "            \"\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(                 #remove the footer\n",
    "            col(\"clean_text\"),\n",
    "            r\"(?is)\\*\\*\\*\\s*END OF.*?\\*\\*\\*.*$\",\n",
    "            \"\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"clean_text\", lower(col(\"clean_text\")))         #convert the text to lower case\n",
    "    .withColumn(\n",
    "        \"clean_text\",\n",
    "        regexp_replace(col(\"clean_text\"), r\"[^a-z\\s]\", \" \")     #remove the punctuations\n",
    "    )\n",
    "    .withColumn(\"words\", split(col(\"clean_text\"), r\"\\s+\"))      #tokenize into words array\n",
    ")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"tokens\")\n",
    "books_clean = remover.transform(books_clean)\n",
    "\n",
    "books_clean = books_clean.select(\"file_name\", \"tokens\")  # keep file_name and tokenized words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d521a06f-e53b-412c-ad46-ba78ddab35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book count =  425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 14:27:00 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 5): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "|file_name|                                                                                              tokens|\n",
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "|   10.txt|[, old, testament, king, james, version, bible, first, book, moses, called, genesis, beginning, g...|\n",
      "|  101.txt|[, hacker, crackdown, law, disorder, electronic, frontier, bruce, sterling, contents, preface, el...|\n",
      "|  102.txt|[, produced, anonymous, volunteer, tragedy, pudd, nhead, wilson, mark, twain, whisper, reader, ch...|\n",
      "|  103.txt|[, around, world, eighty, days, contents, chapter, phileas, fogg, passepartout, accept, one, mast...|\n",
      "|  104.txt|[, inaugural, address, franklin, delano, roosevelt, given, washington, d, c, march, th, president...|\n",
      "+---------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"book count = \", books_clean.count())\n",
    "books_clean.show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433d97cf-ed37-4780-9deb-f54e2af8f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[file_name: string, text: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148992d9-f80c-48ba-9192-18d91c639c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.unpersist(blocking=True) if books_df.is_cached else None\n",
    "del books_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab4f612-e0b5-4603-89a5-fd2bc24a032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731707a3-5e93-4182-b3b4-c2b728fb4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_clean = books_clean.repartition(6, \"file_name\").cache()\n",
    "books_clean.count()   # materialize cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0c440-7f83-451f-af13-c965b651453d",
   "metadata": {},
   "source": [
    "2. TF- IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c38a43-2750-45ef-8c3a-2163d99b6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import udf, col, lit, explode\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import explode, countDistinct, log, lit, col, count, size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47288e95-5ace-4d03-b0bf-8abe0fc3ed77",
   "metadata": {},
   "source": [
    "### Calculate the Term Frequency (TF) of each word in each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9adfe3-a1e7-4707-bddc-8d6b5e3d3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, count\n",
    "\n",
    "tf_df = (\n",
    "    books_clean\n",
    "    .select(\"file_name\", explode(col(\"tokens\")).alias(\"word\"))\n",
    "    .groupBy(\"file_name\", \"word\")\n",
    "    .agg(count(\"*\").alias(\"term_count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07dac251-74ec-4427-b445-51cc351924e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+\n",
      "|file_name|word     |term_count|\n",
      "+---------+---------+----------+\n",
      "|102.txt  |en       |344       |\n",
      "|102.txt  |de       |298       |\n",
      "|102.txt  |tom      |294       |\n",
      "|102.txt  |said     |252       |\n",
      "|102.txt  |wilson   |223       |\n",
      "|102.txt  |one      |202       |\n",
      "|102.txt  |dat      |156       |\n",
      "|102.txt  |got      |136       |\n",
      "|102.txt  |man      |117       |\n",
      "|102.txt  |old      |103       |\n",
      "|112.txt  |one      |92        |\n",
      "|112.txt  |gretchen |84        |\n",
      "|112.txt  |jurgen   |76        |\n",
      "|112.txt  |professor|65        |\n",
      "|112.txt  |back     |54        |\n",
      "|112.txt  |said     |48        |\n",
      "|112.txt  |like     |46        |\n",
      "|112.txt  |time     |46        |\n",
      "|112.txt  |hand     |45        |\n",
      "|112.txt  |viola    |42        |\n",
      "+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"file_name\").orderBy(desc(\"term_count\"))\n",
    "\n",
    "top10_per_book = (\n",
    "    tf_df\n",
    "    .withColumn(\"rank\", row_number().over(window_spec))  # assign rank per book\n",
    "    .filter(\"rank <= 10\")  # keep top 10\n",
    "    .drop(\"rank\")  # remove helper column\n",
    ")\n",
    "\n",
    "top10_per_book.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01798262-856a-4b3f-8934-5a3d2517742f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893d689a-3c61-4c73-96a0-2ac18b36fc89",
   "metadata": {},
   "source": [
    "### Calculate the Inverse Document Frequency (IDF) for each word across all books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69aba7c9-7683-437d-9dfc-a574f0f26ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files counts N  425\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "N = tf_df.select(\"file_name\").distinct().count()\n",
    "\n",
    "print(\"Total files counts N \", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def249bf-317a-4955-957e-308005f384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 6) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|word        |doc_freq|\n",
      "+------------+--------+\n",
      "|spirit      |339     |\n",
      "|doubt       |342     |\n",
      "|matters     |299     |\n",
      "|every       |395     |\n",
      "|rewritten   |15      |\n",
      "|eye         |347     |\n",
      "|ago         |352     |\n",
      "|still       |387     |\n",
      "|del         |44      |\n",
      "|build       |245     |\n",
      "|old         |385     |\n",
      "|honeysuckles|4       |\n",
      "|palings     |21      |\n",
      "|stood       |358     |\n",
      "|boxes       |169     |\n",
      "|grew        |324     |\n",
      "|made        |396     |\n",
      "|symbol      |98      |\n",
      "|chief       |302     |\n",
      "|lofty       |168     |\n",
      "+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Number of files in which a word appeared\n",
    "\n",
    "df_word = (\n",
    "    tf_df            #term frquency df\n",
    "    .groupBy(\"word\")  #group by word\n",
    "    .agg(countDistinct(\"file_name\").alias(\"doc_freq\"))  #count unique documents\n",
    ")\n",
    "\n",
    "df_word.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1813ff84-9069-4db6-800f-eaf708ad3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute IDF \n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "idf_df = df_word.withColumn(\n",
    "    \"idf\",\n",
    "    log(lit(N) / (col(\"doc_freq\") + lit(1)))    # compute log(N/df)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718b8709-4206-4408-b68c-5d7c7dc06603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 6) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------------+\n",
      "|word        |doc_freq|idf                |\n",
      "+------------+--------+-------------------+\n",
      "|spirit      |339     |0.22314355131420976|\n",
      "|doubt       |342     |0.21435872175847698|\n",
      "|matters     |299     |0.3483066942682158 |\n",
      "|every       |395     |0.07067495766993637|\n",
      "|rewritten   |15      |3.2795004466846356 |\n",
      "|eye         |347     |0.19988668914994248|\n",
      "|ago         |352     |0.1856211119911201 |\n",
      "|still       |387     |0.09108382930114349|\n",
      "|del         |44      |2.245426679154097  |\n",
      "|build       |245     |0.5467576329920539 |\n",
      "|old         |385     |0.0962517994595859 |\n",
      "|honeysuckles|4       |4.442651256490317  |\n",
      "|palings     |21      |2.961046715566101  |\n",
      "|stood       |358     |0.16876678043613805|\n",
      "|boxes       |169     |0.9162907318741551 |\n",
      "|grew        |324     |0.26826398659467937|\n",
      "|made        |396     |0.0681528882372264 |\n",
      "|symbol      |98      |1.456969318789827  |\n",
      "|chief       |302     |0.3383563634150477 |\n",
      "|lofty       |168     |0.9221904540013435 |\n",
      "+------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "idf_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a9da7-0271-4119-a5a4-4229ee616951",
   "metadata": {},
   "source": [
    "### Compute the TF-IDF score for each word in each book (TF * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d250c74-96e3-4e33-9f05-88ed572c773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tf_idf_df = (\n",
    "    tf_df     # term frequency data\n",
    "    .join(idf_df.select(\"word\", \"idf\"),  # join with IDF\n",
    "          on=\"word\", how=\"inner\")  # join on word\n",
    "    .withColumn(\"tf_idf\", # create TF-IDF column\n",
    "                col(\"term_count\") * col(\"idf\"))   # multiply TF and IDF\n",
    "    .select(\"file_name\", \"word\", \"tf_idf\")  # select final columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e25f7bad-fd8a-46ce-b98d-742c749489ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=========>                                                (1 + 5) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|file_name|word       |tf_idf            |\n",
      "+---------+-----------+------------------+\n",
      "|38.txt   |aaargh     |10.717883976728944|\n",
      "|200.txt  |aad        |5.358941988364472 |\n",
      "|80.txt   |aag        |5.358941988364472 |\n",
      "|200.txt  |aak        |5.358941988364472 |\n",
      "|124.txt  |abadias    |5.358941988364472 |\n",
      "|14.txt   |abaiang    |4.260329699696362 |\n",
      "|25.txt   |abaiang    |4.260329699696362 |\n",
      "|48.txt   |abaiang    |4.260329699696362 |\n",
      "|180.txt  |abaiang    |4.260329699696362 |\n",
      "|87.txt   |abaiang    |4.260329699696362 |\n",
      "|200.txt  |abailard   |4.953476880256307 |\n",
      "|267.txt  |abailard   |4.953476880256307 |\n",
      "|200.txt  |abaissement|5.358941988364472 |\n",
      "|24.txt   |abandonedly|5.358941988364472 |\n",
      "|200.txt  |abaris     |5.358941988364472 |\n",
      "|228.txt  |abas       |24.767384401281536|\n",
      "|227.txt  |abas       |9.906953760512614 |\n",
      "|102.txt  |abashed    |1.7346010553881064|\n",
      "|172.txt  |abashed    |1.7346010553881064|\n",
      "|224.txt  |abashed    |3.4692021107762128|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "tf_idf_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70bd62a2-230a-4733-934c-8708a819d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "del tf_df \n",
    "del idf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4722f89-3dad-4188-97b3-62755a8cfb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "2409637"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df = tf_idf_df.repartition(6, \"file_name\").cache() \n",
    "tf_idf_df.count() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a7f73-143a-4cdd-969f-3e97c16b134d",
   "metadata": {},
   "source": [
    "### Book Similaritiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50f10ae1-a67a-4716-815b-d7b66c8dee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c523cf25-99a4-4840-94f4-5aa1681b6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tfidf_vector_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ddd343-53ae-44d5-b284-dd1a84a5ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, sqrt, col\n",
    "\n",
    "norm_df = (\n",
    "    tf_idf_df\n",
    "    .withColumn(\"square\", col(\"tf_idf\") * col(\"tf_idf\"))  # square value\n",
    "    .groupBy(\"file_name\")   # group per book\n",
    "    .agg(spark_sum(\"square\").alias(\"sum_sq\"))  # sum of squares\n",
    "    .withColumn(\"norm\", sqrt(col(\"sum_sq\")))   #compute norm\n",
    "    .select(\"file_name\", \"norm\")  # keep needed cols\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90a23bca-9926-40c5-9c31-993f923cc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_df = (\n",
    "    tf_idf_df.alias(\"a\")\n",
    "    .join(tf_idf_df.alias(\"b\"), on=\"word\")  # match common words\n",
    "    .filter(col(\"a.file_name\") < col(\"b.file_name\"))  # avoid duplicates\n",
    "    .withColumn(\"product\", col(\"a.tf_idf\") * col(\"b.tf_idf\"))# multiply values\n",
    "    .groupBy(\"a.file_name\", \"b.file_name\")  # group book pairs\n",
    "    .agg(spark_sum(\"product\").alias(\"dot_product\"))  # compute dot\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40fa0535-71f5-4ce5-b222-d545cca9b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = (\n",
    "    dot_df\n",
    "    .join(norm_df.alias(\"n1\"), dot_df[\"a.file_name\"] == col(\"n1.file_name\"))\n",
    "    .join(norm_df.alias(\"n2\"), dot_df[\"b.file_name\"] == col(\"n2.file_name\"))\n",
    "    .withColumn(\n",
    "        \"cosine_similarity\",\n",
    "        col(\"dot_product\") / (col(\"n1.norm\") * col(\"n2.norm\"))  # cosine formula\n",
    "    )\n",
    "    .select(\n",
    "        col(\"a.file_name\").alias(\"book1\"),\n",
    "        col(\"b.file_name\").alias(\"book2\"),\n",
    "        \"cosine_similarity\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44c85178-2be1-4560-a333-d9a20f0e5c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|similar_book|cosine_similarity  |\n",
      "+------------+-------------------+\n",
      "|30.txt      |0.9999885553852694 |\n",
      "|58.txt      |0.449803428012394  |\n",
      "|26.txt      |0.41985814420968676|\n",
      "|357.txt     |0.32755879652499764|\n",
      "|109.txt     |0.28829094553856716|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "target_book = \"10.txt\"\n",
    "\n",
    "top5 = (\n",
    "    similarity_df\n",
    "    .filter((col(\"book1\") == target_book) | \n",
    "            (col(\"book2\") == target_book)) # select related pairs\n",
    "    .withColumn(\n",
    "        \"similar_book\",\n",
    "        expr(f\"CASE WHEN book1 = '{target_book}' THEN book2 ELSE book1 END\")\n",
    "    )\n",
    "    .orderBy(desc(\"cosine_similarity\"))   # sort by similarity\n",
    "    .select(\"similar_book\", \"cosine_similarity\")    # final columns\n",
    "    .limit(5)  # top 5 only\n",
    ")\n",
    "\n",
    "top5.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12bf55-39ae-4a82-824d-0e2044050019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ece8d9-ed91-4b4e-802f-a6e260dc46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824c17d-30b6-4912-a05c-84ef4d3fe1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bf51b5-9094-408d-ba30-658f73d56f97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d29b3-ca4e-4a56-b618-3284c3fe9192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd55c49-693e-40c7-bf5a-903ce796f7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f4715-568b-4325-900d-85a3caa843fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2515b77-bff6-4b76-bede-55aeb7e6096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509b04d-32b3-4b17-ab4e-6ed025b09762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973fee9-6c37-4b79-8347-929df7d9dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
